apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: prometheus-k8s-rules
  namespace: monitoring
spec:
  groups:
  - name: k8s.rules
    rules:
    - expr: |
        sum(rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container_name!=""}[5m])) by (namespace)
      record: namespace:container_cpu_usage_seconds_total:sum_rate
    - expr: |
        sum by (namespace, pod_name, container_name) (
          rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container_name!=""}[5m])
        )
      record: namespace_pod_name_container_name:container_cpu_usage_seconds_total:sum_rate
    - expr: |
        sum(container_memory_usage_bytes{job="kubelet", image!="", container_name!=""}) by (namespace)
      record: namespace:container_memory_usage_bytes:sum
    - expr: |
        sum by (namespace, label_name) (
           sum(rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container_name!=""}[5m])) by (namespace, pod_name)
         * on (namespace, pod_name) group_left(label_name)
           label_replace(kube_pod_labels{job="kube-state-metrics"}, "pod_name", "$1", "pod", "(.*)")
        )
      record: namespace_name:container_cpu_usage_seconds_total:sum_rate
    - expr: |
        sum by (namespace, label_name) (
          sum(container_memory_usage_bytes{job="kubelet",image!="", container_name!=""}) by (pod_name, namespace)
        * on (namespace, pod_name) group_left(label_name)
          label_replace(kube_pod_labels{job="kube-state-metrics"}, "pod_name", "$1", "pod", "(.*)")
        )
      record: namespace_name:container_memory_usage_bytes:sum
    - expr: |
        sum by (namespace, label_name) (
          sum(kube_pod_container_resource_requests_memory_bytes{job="kube-state-metrics"}) by (namespace, pod)
        * on (namespace, pod) group_left(label_name)
          label_replace(kube_pod_labels{job="kube-state-metrics"}, "pod_name", "$1", "pod", "(.*)")
        )
      record: namespace_name:kube_pod_container_resource_requests_memory_bytes:sum
    - expr: |
        sum by (namespace, label_name) (
          sum(kube_pod_container_resource_requests_cpu_cores{job="kube-state-metrics"} and on(pod) kube_pod_status_scheduled{condition="true"}) by (namespace, pod)
        * on (namespace, pod) group_left(label_name)
          label_replace(kube_pod_labels{job="kube-state-metrics"}, "pod_name", "$1", "pod", "(.*)")
        )
      record: namespace_name:kube_pod_container_resource_requests_cpu_cores:sum
  - name: kube-apiserver.rules
    rules:
    - expr: |
        histogram_quantile(0.99, sum(rate(apiserver_request_latencies_bucket{job="apiserver"}[5m])) without(instance, pod)) / 1e+06
      labels:
        quantile: "0.99"
      record: cluster_quantile:apiserver_request_latencies:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(apiserver_request_latencies_bucket{job="apiserver"}[5m])) without(instance, pod)) / 1e+06
      labels:
        quantile: "0.9"
      record: cluster_quantile:apiserver_request_latencies:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(apiserver_request_latencies_bucket{job="apiserver"}[5m])) without(instance, pod)) / 1e+06
      labels:
        quantile: "0.5"
      record: cluster_quantile:apiserver_request_latencies:histogram_quantile
  - name: node.rules
    rules:
    - expr: sum(min(kube_pod_info) by (node))
      record: ':kube_pod_info_node_count:'
    - expr: |
        max(label_replace(kube_pod_info{job="kube-state-metrics"}, "pod", "$1", "pod", "(.*)")) by (node, namespace, pod)
      record: 'node_namespace_pod:kube_pod_info:'
    - expr: |
        count by (node) (sum by (node, cpu) (
          node_cpu{job="node-exporter"}
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
        ))
      record: node:node_num_cpu:sum
    - expr: |
        1 - avg(rate(node_cpu{job="node-exporter",mode="idle"}[1m]))
      record: :node_cpu_utilisation:avg1m
    - expr: |
        1 - avg by (node) (
          rate(node_cpu{job="node-exporter",mode="idle"}[1m])
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:)
      record: node:node_cpu_utilisation:avg1m
    - expr: |
        sum(node_load1{job="node-exporter"})
        /
        sum(node:node_num_cpu:sum)
      record: ':node_cpu_saturation_load1:'
    - expr: |
        sum by (node) (
          node_load1{job="node-exporter"}
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
        )
        /
        node:node_num_cpu:sum
      record: 'node:node_cpu_saturation_load1:'
    - expr: |
        1 -
        sum(node_memory_MemFree{job="node-exporter"} + node_memory_Cached{job="node-exporter"} + node_memory_Buffers{job="node-exporter"})
        /
        sum(node_memory_MemTotal{job="node-exporter"})
      record: ':node_memory_utilisation:'
    - expr: |
        sum(node_memory_MemFree{job="node-exporter"} + node_memory_Cached{job="node-exporter"} + node_memory_Buffers{job="node-exporter"})
      record: :node_memory_MemFreeCachedBuffers:sum
    - expr: |
        sum(node_memory_MemTotal{job="node-exporter"})
      record: :node_memory_MemTotal:sum
    - expr: |
        sum by (node) (
          (node_memory_MemFree{job="node-exporter"} + node_memory_Cached{job="node-exporter"} + node_memory_Buffers{job="node-exporter"})
          * on (namespace, pod) group_left(node)
            node_namespace_pod:kube_pod_info:
        )
      record: node:node_memory_bytes_available:sum
    - expr: |
        sum by (node) (
          node_memory_MemTotal{job="node-exporter"}
          * on (namespace, pod) group_left(node)
            node_namespace_pod:kube_pod_info:
        )
      record: node:node_memory_bytes_total:sum
    - expr: |
        (node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)
        /
        scalar(sum(node:node_memory_bytes_total:sum))
      record: node:node_memory_utilisation:ratio
    - expr: |
        1e3 * sum(
          (rate(node_vmstat_pgpgin{job="node-exporter"}[1m])
         + rate(node_vmstat_pgpgout{job="node-exporter"}[1m]))
        )
      record: :node_memory_swap_io_bytes:sum_rate
    - expr: |
        1 -
        sum by (node) (
          (node_memory_MemFree{job="node-exporter"} + node_memory_Cached{job="node-exporter"} + node_memory_Buffers{job="node-exporter"})
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
        )
        /
        sum by (node) (
          node_memory_MemTotal{job="node-exporter"}
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
        )
      record: 'node:node_memory_utilisation:'
    - expr: |
        1 - (node:node_memory_bytes_available:sum / node:node_memory_bytes_total:sum)
      record: 'node:node_memory_utilisation_2:'
    - expr: |
        1e3 * sum by (node) (
          (rate(node_vmstat_pgpgin{job="node-exporter"}[1m])
         + rate(node_vmstat_pgpgout{job="node-exporter"}[1m]))
         * on (namespace, pod) group_left(node)
           node_namespace_pod:kube_pod_info:
        )
      record: node:node_memory_swap_io_bytes:sum_rate
    - expr: |
        avg(irate(node_disk_io_time_ms{job="node-exporter",device=~"(sd|xvd|nvme).+"}[1m]) / 1e3)
      record: :node_disk_utilisation:avg_irate
    - expr: |
        avg by (node) (
          irate(node_disk_io_time_ms{job="node-exporter",device=~"(sd|xvd|nvme).+"}[1m]) / 1e3
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
        )
      record: node:node_disk_utilisation:avg_irate
    - expr: |
        avg(irate(node_disk_io_time_weighted{job="node-exporter",device=~"(sd|xvd|nvme).+"}[1m]) / 1e3)
      record: :node_disk_saturation:avg_irate
    - expr: |
        avg by (node) (
          irate(node_disk_io_time_weighted{job="node-exporter",device=~"(sd|xvd|nvme).+"}[1m]) / 1e3
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
        )
      record: node:node_disk_saturation:avg_irate
    - expr: |
        max by (namespace, pod, device, instance) ((node_filesystem_size{fstype=~"ext[234]|btrfs|xfs|zfs|vfat|tmpfs"}
        - node_filesystem_avail{fstype=~"ext[234]|btrfs|xfs|zfs|vfat|tmpfs"})
        / node_filesystem_size{fstype=~"ext[234]|btrfs|xfs|zfs|vfat|tmpfs"})
      record: 'node:node_filesystem_usage:'
    - expr: |
        max by (namespace, pod, device, instance) (node_filesystem_avail{fstype=~"ext[234]|btrfs|xfs|zfs|vfat|tmpfs"} / node_filesystem_size{fstype=~"ext[234]|btrfs|xfs|zfs|vfat|tmpfs"})
      record: 'node:node_filesystem_avail:'
    - expr: |
        sum(irate(node_network_receive_bytes{job="node-exporter",device=~"eth[[:digit:]]+|en.*"}[1m])) +
        sum(irate(node_network_transmit_bytes{job="node-exporter",device=~"eth[[:digit:]]+|en.*"}[1m]))
      record: :node_net_utilisation:sum_irate
    - expr: |
        sum by (node) (
          (irate(node_network_receive_bytes{job="node-exporter",device=~"eth[[:digit:]]+|en.*"}[1m]) +
          irate(node_network_transmit_bytes{job="node-exporter",device=~"eth[[:digit:]]+|en.*"}[1m]))
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
        )
      record: node:node_net_utilisation:sum_irate
    - expr: |
        sum(irate(node_network_receive_drop{job="node-exporter",device=~"eth[[:digit:]]+|en.*"}[1m])) +
        sum(irate(node_network_transmit_drop{job="node-exporter",device=~"eth[[:digit:]]+|en.*"}[1m]))
      record: :node_net_saturation:sum_irate
    - expr: |
        sum by (node) (
          (irate(node_network_receive_drop{job="node-exporter",device=~"eth[[:digit:]]+|en.*"}[1m]) +
          irate(node_network_transmit_drop{job="node-exporter",device=~"eth[[:digit:]]+|en.*"}[1m]))
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
        )
      record: node:node_net_saturation:sum_irate
    - expr: |
        process_open_fds / process_max_fds
      record: instance:fd_utilization
  - name: kube-prometheus-node-recording.rules
    rules:
    - expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[3m])) BY (instance)
      record: instance:node_cpu:rate:sum
    - expr: sum((node_filesystem_size{mountpoint="/"} - node_filesystem_free{mountpoint="/"}))
        BY (instance)
      record: instance:node_filesystem_usage:sum
    - expr: sum(rate(node_network_receive_bytes[3m])) BY (instance)
      record: instance:node_network_receive_bytes:rate:sum
    - expr: sum(rate(node_network_transmit_bytes[3m])) BY (instance)
      record: instance:node_network_transmit_bytes:rate:sum
    - expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[5m])) WITHOUT (cpu, mode)
        / ON(instance) GROUP_LEFT() count(sum(node_cpu) BY (instance, cpu)) BY (instance)
      record: instance:node_cpu:ratio
    - expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[5m]))
      record: cluster:node_cpu:sum_rate5m
    - expr: cluster:node_cpu:rate5m / count(sum(node_cpu) BY (instance, cpu))
      record: cluster:node_cpu:ratio
  - name: node_exporter-16-bcache
    rules:
    - expr: node_bcache_cache_read_races
      record: node_bcache_cache_read_races_total
  - name: node_exporter-16-buddyinfo
    rules:
    - expr: node_buddyinfo_blocks
      record: node_buddyinfo_count
  - name: node_exporter-16-stat
    rules:
    - expr: node_boot_time_seconds
      record: node_boot_time
    - expr: node_context_switches_total
      record: node_context_switches
    - expr: node_forks_total
      record: node_forks
    - expr: node_intr_total
      record: node_intr
  - name: node_exporter-16-cpu
    rules:
    - expr: label_replace(node_cpu_seconds_total, "cpu", "$1", "cpu", "cpu(.+)")
      record: node_cpu
  - name: node_exporter-16-diskstats
    rules:
    - expr: node_disk_read_bytes_total
      record: node_disk_bytes_read
    - expr: node_disk_written_bytes_total
      record: node_disk_bytes_written
    - expr: node_disk_io_time_seconds_total * 1000
      record: node_disk_io_time_ms
    - expr: node_disk_io_time_weighted_seconds_total
      record: node_disk_io_time_weighted
    - expr: node_disk_reads_completed_total
      record: node_disk_reads_completed
    - expr: node_disk_reads_merged_total
      record: node_disk_reads_merged
    - expr: node_disk_read_time_seconds_total * 1000
      record: node_disk_read_time_ms
    - expr: node_disk_writes_completed_total
      record: node_disk_writes_completed
    - expr: node_disk_writes_merged_total
      record: node_disk_writes_merged
    - expr: node_disk_write_time_seconds_total * 1000
      record: node_disk_write_time_ms
  - name: node_exporter-16-filesystem
    rules:
    - expr: node_filesystem_free_bytes
      record: node_filesystem_free
    - expr: node_filesystem_avail_bytes
      record: node_filesystem_avail
    - expr: node_filesystem_size_bytes
      record: node_filesystem_size
  - name: node_exporter-16-infiniband
    rules:
    - expr: node_infiniband_port_data_received_bytes_total
      record: node_infiniband_port_data_received_bytes
    - expr: node_infiniband_port_data_transmitted_bytes_total
      record: node_infiniband_port_data_transmitted_bytes
  - name: node_exporter-16-interrupts
    rules:
    - expr: node_interrupts_total
      record: node_interrupts
  - name: node_exporter-16-memory
    rules:
    - expr: node_memory_Active_bytes
      record: node_memory_Active
    - expr: node_memory_Active_anon_bytes
      record: node_memory_Active_anon
    - expr: node_memory_Active_file_bytes
      record: node_memory_Active_file
    - expr: node_memory_AnonHugePages_bytes
      record: node_memory_AnonHugePages
    - expr: node_memory_AnonPages_bytes
      record: node_memory_AnonPages
    - expr: node_memory_Bounce_bytes
      record: node_memory_Bounce
    - expr: node_memory_Buffers_bytes
      record: node_memory_Buffers
    - expr: node_memory_Cached_bytes
      record: node_memory_Cached
    - expr: node_memory_CommitLimit_bytes
      record: node_memory_CommitLimit
    - expr: node_memory_Committed_AS_bytes
      record: node_memory_Committed_AS
    - expr: node_memory_DirectMap2M_bytes
      record: node_memory_DirectMap2M
    - expr: node_memory_DirectMap4k_bytes
      record: node_memory_DirectMap4k
    - expr: node_memory_Dirty_bytes
      record: node_memory_Dirty
    - expr: node_memory_HardwareCorrupted_bytes
      record: node_memory_HardwareCorrupted
    - expr: node_memory_Hugepagesize_bytes
      record: node_memory_Hugepagesize
    - expr: node_memory_Inactive_bytes
      record: node_memory_Inactive
    - expr: node_memory_Inactive_anon_bytes
      record: node_memory_Inactive_anon
    - expr: node_memory_Inactive_file_bytes
      record: node_memory_Inactive_file
    - expr: node_memory_KernelStack_bytes
      record: node_memory_KernelStack
    - expr: node_memory_Mapped_bytes
      record: node_memory_Mapped
    - expr: node_memory_MemAvailable_bytes
      record: node_memory_MemAvailable
    - expr: node_memory_MemFree_bytes
      record: node_memory_MemFree
    - expr: node_memory_MemTotal_bytes
      record: node_memory_MemTotal
    - expr: node_memory_Mlocked_bytes
      record: node_memory_Mlocked
    - expr: node_memory_NFS_Unstable_bytes
      record: node_memory_NFS_Unstable
    - expr: node_memory_PageTables_bytes
      record: node_memory_PageTables
    - expr: node_memory_Shmem_bytes
      record: node_memory_Shmem
    - expr: node_memory_Slab_bytes
      record: node_memory_Slab
    - expr: node_memory_SReclaimable_bytes
      record: node_memory_SReclaimable
    - expr: node_memory_SUnreclaim_bytes
      record: node_memory_SUnreclaim
    - expr: node_memory_SwapCached_bytes
      record: node_memory_SwapCached
    - expr: node_memory_SwapFree_bytes
      record: node_memory_SwapFree
    - expr: node_memory_SwapTotal_bytes
      record: node_memory_SwapTotal
    - expr: node_memory_Unevictable_bytes
      record: node_memory_Unevictable
    - expr: node_memory_VmallocChunk_bytes
      record: node_memory_VmallocChunk
    - expr: node_memory_VmallocTotal_bytes
      record: node_memory_VmallocTotal
    - expr: node_memory_VmallocUsed_bytes
      record: node_memory_VmallocUsed
    - expr: node_memory_Writeback_bytes
      record: node_memory_Writeback
    - expr: node_memory_WritebackTmp_bytes
      record: node_memory_WritebackTmp
  - name: node_exporter-16-network
    rules:
    - expr: node_network_receive_bytes_total
      record: node_network_receive_bytes
    - expr: node_network_receive_compressed_total
      record: node_network_receive_compressed
    - expr: node_network_receive_drop_total
      record: node_network_receive_drop
    - expr: node_network_receive_errs_total
      record: node_network_receive_errs
    - expr: node_network_receive_fifo_total
      record: node_network_receive_fifo
    - expr: node_network_receive_frame_total
      record: node_network_receive_frame
    - expr: node_network_receive_multicast_total
      record: node_network_receive_multicast
    - expr: node_network_receive_packets_total
      record: node_network_receive_packets
    - expr: node_network_transmit_bytes_total
      record: node_network_transmit_bytes
    - expr: node_network_transmit_compressed_total
      record: node_network_transmit_compressed
    - expr: node_network_transmit_drop_total
      record: node_network_transmit_drop
    - expr: node_network_transmit_errs_total
      record: node_network_transmit_errs
    - expr: node_network_transmit_fifo_total
      record: node_network_transmit_fifo
    - expr: node_network_transmit_frame_total
      record: node_network_transmit_frame
    - expr: node_network_transmit_multicast_total
      record: node_network_transmit_multicast
    - expr: node_network_transmit_packets_total
      record: node_network_transmit_packets
  - name: node_exporter-16-nfs
    rules:
    - expr: node_nfs_connections_total
      record: node_nfs_net_connections
    - expr: node_nfs_packets_total
      record: node_nfs_net_reads
    - expr: label_replace(label_replace(node_nfs_requests_total, "proto", "$1", "version",
        "(.+)"), "method", "$1", "procedure", "(.+)")
      record: node_nfs_procedures
    - expr: node_nfs_rpc_authentication_refreshes_total
      record: node_nfs_rpc_authentication_refreshes
    - expr: node_nfs_rpcs_total
      record: node_nfs_rpc_operations
    - expr: node_nfs_rpc_retransmissions_total
      record: node_nfs_rpc_retransmissions
  - name: node_exporter-16-textfile
    rules:
    - expr: node_textfile_mtime_seconds
      record: node_textfile_mtime
  - name: kubernetes-absent
    rules:
    - alert: AlertmanagerDown
      annotations:
        message: Alertmanager has disappeared from Prometheus target discovery.
        doc: This alert fires if Prometheus target discovery was not able to reach AlertManager in the last 15 minutes.
      expr: |
        absent(up{job="alertmanager-main"} == 1)
      for: 15m
      labels:
        severity: critical
    - alert: KubeAPIDown
      annotations:
        message: KubeAPI has disappeared from Prometheus target discovery.
        doc: This alert fires if Prometheus target discovery was not able to reach kube-apiserver in the last 15 minutes.
      expr: |
        absent(up{job="apiserver"} == 1)
      for: 15m
      labels:
        severity: critical
    - alert: KubeStateMetricsDown
      annotations:
        message: KubeStateMetrics has disappeared from Prometheus target discovery.
        doc: This alert fires if Prometheus target discovery was not able to reach kube-state-metrics in the last 15 minutes.
      expr: |
        absent(up{job="kube-state-metrics"} == 1)
      for: 15m
      labels:
        severity: critical
    - alert: KubeletDown
      annotations:
        message: Kubelet has disappeared from Prometheus target discovery.
        doc: This alert fires if Prometheus target discovery was not able to reach the kubelet in the last 15 minutes.
      expr: |
        absent(up{job="kubelet"} == 1)
      for: 15m
      labels:
        severity: critical
    - alert: NodeExporterDown
      annotations:
        message: NodeExporter has disappeared from Prometheus target discovery.
        doc: 'This alert fires if Prometheus target discovery was not able to
          reach node-exporter in the last 15 minutes.'
      expr: |
        absent(up{job="node-exporter"} == 1)
      for: 15m
      labels:
        severity: critical
    - alert: PrometheusDown
      annotations:
        message: Prometheus has disappeared from Prometheus target discovery.
        doc: 'This alert fires if Prometheus target discovery was not able to
          reach Prometheus in the last 15 minutes.'
      expr: |
        absent(up{job="prometheus-k8s"} == 1)
      for: 15m
      labels:
        severity: critical
    - alert: PrometheusOperatorDown
      annotations:
        message: PrometheusOperator has disappeared from Prometheus target discovery.
        doc: 'This alert fires if Prometheus target discovery was not able to
          reach the Prometheus Operator in the last 15 minutes.'
      expr: |
        absent(up{job="prometheus-operator"} == 1)
      for: 15m
      labels:
        severity: critical
  - name: kubernetes-apps
    rules:
    - alert: KubePodCrashLooping
      annotations:
        message: '{{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
          }}) is restarting {{ printf "%.2f" $value }} / second'
        doc: 'This alert fires if the per-second rate of the total number of
          restart of a given pod in a 15 minutes time window was above 0 in the
          last hour, i.e. the pod is stuck in a crash loop.'
      expr: |
        rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[15m]) > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubePodNotReady
      annotations:
        message: '{{ $labels.namespace }}/{{ $labels.pod }} is not ready.'
        doc: 'This alert fires if at least one pod was stuck in the Pending or
          Unknown phase in the last hour.'
      expr: |
        sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown"}) > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubeDeploymentGenerationMismatch
      annotations:
        message: 'Deployment {{ $labels.namespace }}/{{ $labels.deployment }}
          generation mismatch'
        doc: "This alert fires if in the last hour a deployment's observed
        generation (the revision number recorded in the object status) was
        different from the metadata generation (the revision number in the
        deployment metadata)."
      expr: |
        kube_deployment_status_observed_generation{job="kube-state-metrics"}
          !=
        kube_deployment_metadata_generation{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
    - alert: KubeDeploymentReplicasMismatch
      annotations:
        message: 'Deployment {{ $labels.namespace }}/{{ $labels.deployment }}
          replica mismatch'
        doc: "This alert fires if a deployment's replicas number specification
          was different from the available replicas in the last hour."
      expr: |
        kube_deployment_spec_replicas{job="kube-state-metrics"}
          !=
        kube_deployment_status_replicas_available{job="kube-state-metrics"}
      for: 1h
      labels:
        severity: warning
    - alert: KubeStatefulSetReplicasMismatch
      annotations:
        message: 'StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }}
          replica mismatch'
        doc: "This alert fires if a deployment's replicas number specification
          was different from the available replicas in the last hour."
      expr: |
        kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
          !=
        kube_statefulset_status_replicas{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetGenerationMismatch
      annotations:
        message: 'StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }}
          generation mismatch'
        doc: "This alert fires if a StatefulSet's replicas number specification
          was different from the available replicas in the 15 minutes."
      expr: |
        kube_statefulset_status_observed_generation{job="kube-state-metrics"}
          !=
        kube_statefulset_metadata_generation{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
    - alert: KubeDaemonSetRolloutStuck
      annotations:
        message: 'Only {{ printf "%.2f" $value }}% of desired pods scheduled and
          ready for daemon set {{ $labels.namespace }}/{{ $labels.daemonset }}'
        doc: "This alert fires if the percentage of DaemonSet in the ready phase
          was less than 100% in the last 15 minutes."
      expr: |
        kube_daemonset_status_number_ready{job="kube-state-metrics"}
          /
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"} * 100 < 100
      for: 15m
      labels:
        severity: warning
    - alert: KubeDaemonSetNotScheduled
      annotations:
        message: 'A number of pods of daemonset {{ $labels.namespace }}/{{
          $labels.daemonset }} are not scheduled.'
        doc: "This alert fires if the desired number of scheduled DaemonSet was
          higher than the number of currently scheduled DaemonSet in the last 10
          minutes."
      expr: |
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
          -
        kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"} > 0
      for: 10m
      labels:
        severity: warning
    - alert: KubeDaemonSetMisScheduled
      annotations:
        message: 'A number of pods of daemonset {{ $labels.namespace }}/{{
          $labels.daemonset }} are running where they are not supposed to run.'
        doc: "This alert fires if at least one DaemonSet was running where it
          was not supposed to run in the last 10 minutes."
      expr: |
        kube_daemonset_status_number_misscheduled{job="kube-state-metrics"} > 0
      for: 10m
      labels:
        severity: warning
    - alert: KubeCronJobRunning
      annotations:
        message: 'CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is
          taking more than 1h to complete.'
        doc: " This alert fires if at least one CronJob took more than one hour
          to complete."
      expr: |
        time() - kube_cronjob_next_schedule_time{job="kube-state-metrics"} > 3600
      for: 1h
      labels:
        severity: warning
    - alert: KubeJobCompletion
      annotations:
        message: 'Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more
          than 1h to complete.'
        doc: "This alert fires if at least on Job took more than one hour to
          complete."
      expr: |
        kube_job_spec_completions{service="kube-state-metrics"}
          -
        kube_job_status_succeeded{service="kube-state-metrics"} > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubeJobFailed
      annotations:
        message: 'Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to
          complete.'
        doc: "This alert fires if at least one Job failed in the last hour."
      expr: |
        kube_job_status_failed{service="kube-state-metrics"} > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubeLatestImageTag
      annotations:
        message: 'The image in {{ $labels.container }} container in {{ $labels.namespace }}/{{ $labels.pod }} pod is running with :latest tag'
        doc: "This alert fires if there are images deployed in the cluster tagged with `:latest` and this is really dangerous"
      expr: |
        kube_pod_container_info{image=~".*:latest"} > 0
      for: 1h
      labels:
        severity: warning
  - name: kubernetes-resources
    rules:
    - alert: KubeCPUOvercommit
      annotations:
        message: 'Overcommited CPU resource requests on Pods, cannot tolerate
          node failure.'
        doc: "This alert fires if the cluster-wide CPU requests from pods in the
          last 5 minutes were so high to not tolerate a node failure."
      expr: |
        sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)
          /
        sum(node:node_num_cpu:sum)
          >
        (count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum)
      for: 5m
      labels:
        severity: warning
    - alert: KubeMemOvercommit
      annotations:
        message: 'Overcommited Memory resource requests on Pods, cannot tolerate
          node failure.'
        doc: "This alert fires if the cluster-wide memory requests from pods in
          the last 5 minutes were so high to not tolerate a node failure."
      expr: |
        sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)
          /
        sum(node_memory_MemTotal)
          >
        (count(node:node_num_cpu:sum)-1)
          /
        count(node:node_num_cpu:sum)
      for: 5m
      labels:
        severity: warning
    - alert: KubeCPUOvercommit
      annotations:
        message: 'Overcommited CPU resource request quota on Namespaces.'
        doc: "This alert fires if the hard limit of CPU resources quota in the
          last 5 minutes is more than 150% of the available resources, i.e. the
          hard limit is set too high."
      expr: |
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="requests.cpu"})
          /
        sum(node:node_num_cpu:sum)
          > 1.5
      for: 5m
      labels:
        severity: warning
    - alert: KubeMemOvercommit
      annotations:
        message: 'Overcommited Memory resource request quota on Namespaces.'
        doc: "This alert fires if the hard limit of memory resources quota in
          the last 5 minutes is more than 150% of the available resources, i.e.
          the hard limit is set too high."
      expr: |
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="requests.memory"})
          /
        sum(node_memory_MemTotal{job="node-exporter"})
          > 1.5
      for: 5m
      labels:
        severity: warning
    - alert: KubeQuotaExceeded
      annotations:
        message: '{{ printf "%0.0f" $value }}% usage of {{ $labels.resource }}
          in namespace {{ $labels.namespace }}.'
        doc: "This alert fires if a given resource was used for more than 90% of
          the corresponding hard quota in the last 15 minutes."
      expr: |
        100 * kube_resourcequota{job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        kube_resourcequota{job="kube-state-metrics", type="hard"}
          > 90
      for: 15m
      labels:
        severity: warning
  - name: kubernetes-storage
    rules:
    - alert: KubePersistentVolumeStuck
      annotations:
        message: 'The persistent volume {{ $labels.persistentvolume }} is stuck
          in the {{ $labels.phase }} phase.'
        doc: "This alert fires if a given persisten volume was stuck in the
          Pending or Failed phase in the last hour."
      expr: |
        kube_persistentvolume_status_phase{phase=~"Pending|Failed"} > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubePersistentVolumeUsageCritical
      annotations:
        message: 'The persistent volume claimed by
          {{ $labels.persistentvolumeclaim }} in namespace
          {{ $labels.namespace }} has {{ printf "%0.0f" $value }}% free.'
        doc: "This alert fires if the available space in a given
          PersistentVolumeClaim was less than 10% in the last minute."
      expr: |
        100 * kubelet_volume_stats_available_bytes{job="kubelet"}
          /
        kubelet_volume_stats_capacity_bytes{job="kubelet"}
          < 10
      for: 1m
      labels:
        severity: critical
    - alert: KubePersistentVolumeFullInFourDays
      annotations:
        message: 'Based on recent sampling, the persistent volume claimed by
          {{ $labels.persistentvolumeclaim }} in namespace
          {{ $labels.namespace }} is expected to fill up within four days.'
        doc: "This alert fires if, based on a linear prediction on the
          volume usage in the last 6 hours, the volume will be full in four
          days."
      expr: |
        ((kubelet_volume_stats_used_bytes{job="kubelet"}
          /
        kubelet_volume_stats_total_bytes{job="kubelet"}) > 0.85)
        and
        predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600) < 0
      for: 5m
      labels:
        severity: warning
    - alert: KubePersistentVolumeInodeUsageCritical
      annotations:
        message: 'The persistent volume claimed by
          {{ $labels.persistentvolumeclaim }} in namespace
          {{ $labels.namespace }} has {{ printf "%0.0f" $value }}% inodes free.'
        doc: "This alert fires if the available inodes in a given
          PersistentVolumeClaim were less than 10% in the last minute."
      expr: |
        100 * kubelet_volume_stats_inodes_available{job="kubelet"}
          /
        kubelet_volume_stats_inodes{job="kubelet"}
          < 10
      for: 1m
      labels:
        severity: critical
    - alert: KubePersistentVolumeInodeFullInFourDays
      annotations:
        message: 'Based on recent sampling, the persistent volume claimed by
          {{ $labels.persistentvolumeclaim }} in namespace
          {{ $labels.namespace }} is expected to exhaust its inodes within four
          days.'
        doc: "This alert fires if, based on a linear prediction on the
          inodes usage in the last 6 hours, the volume will exhaust its inodes
          in four days."
      expr: |
        ((kubelet_volume_stats_inodes_used{job="kubelet"}
          /
        kubelet_volume_stats_inodes{job="kubelet"}) > 0.85)
        and
        predict_linear(kubelet_volume_stats_inodes_available{job="kubelet"}[6h], 4 * 24 * 3600) < 0
      for: 5m
      labels:
        severity: warning
  - name: kubernetes-system
    rules:
    - alert: KubeNodeNotReady
      annotations:
        message: '{{ $labels.node }} has been in {{ $labels.condition }}
          condition for more than 15 minutes.'
        doc: "This alert fires if a given node was not in Ready status in the
          last 15 minutes.."
      expr: |
        kube_node_status_condition{job="kube-state-metrics",condition!="Ready",status="true"} == 1
      for: 15m
      labels:
        severity: critical
    - alert: KubeVersionMismatch
      annotations:
        message: 'There are {{ $value }} different versions of Kubernetes
        components running.'
        doc: "This alert fires if the versions of the Kubernetes components were
          mismatching in the last hour."
      expr: |
        count(count(kubernetes_build_info{job!="kube-dns"}) by (gitVersion)) > 1
      for: 1h
      labels:
        severity: warning
    - alert: KubeClientErrors
      annotations:
        message: 'Kubernetes API server client
          {{ $labels.job }}/{{ $labels.instance }} is experiencing
          {{ printf "%0.0f" $value }}% errors.'
        doc: "This alert fires if the Kubernetes API client error responses rate
          calculated in a 5 minutes window was more than 1% in the last 15
          minutes."
      expr: |
        (sum(rate(rest_client_requests_total{code!~"2..|404"}[5m])) by (instance, job)
          /
        sum(rate(rest_client_requests_total[5m])) by (instance, job))
        * 100 > 1
      for: 15m
      labels:
        severity: warning
    - alert: KubeClientErrors
      annotations:
        message: 'Kubernetes API server client
          {{ $labels.job }}/{{ $labels.instance }} is experiencing
          {{ printf "%0.0f" $value }} errors / sec.'
        doc: "This alert fires if the Kubernetes API client error responses rate
          calculated in a 5 minutes window was more than 0.1 errors / sec in the
          last 15 minutes."
      expr: |
        sum(rate(ksm_scrape_error_total{job="kube-state-metrics"}[5m])) by (instance, job) > 0.1
      for: 15m
      labels:
        severity: warning
    - alert: KubeletTooManyPods
      annotations:
        message: 'Kubelet {{$labels.instance}} is running {{$value}} pods, close
          to the limit of 110.'
        doc: "This alert fires if a given kubelet is running more than 100 pods
          and is approaching the hard limit of 110 pods per node."
      expr: |
        kubelet_running_pod_count{job="kubelet"} > 100
      for: 15m
      labels:
        severity: warning
    - alert: KubeAPILatencyHigh
      annotations:
        message: 'The API server has a 99th percentile latency of {{ $value }}
          seconds for {{ $labels.verb }} {{ $labels.resource }}.'
        doc: "This alert fires if the API server 99th percentile latency was
          more than 1 second in the last 10 minutes."
      expr: |
        cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 1
      for: 10m
      labels:
        severity: warning
    - alert: KubeAPILatencyHigh
      annotations:
        message: 'The API server has a 99th percentile latency of {{ $value }}
          seconds for {{ $labels.verb }} {{ $labels.resource }}.'
        doc: "This alert fires if the API server 99th percentile latency was
          more than 4 second in the last 10 minutes."
      expr: |
        cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 4
      for: 10m
      labels:
        severity: critical
    - alert: KubeAPIErrorsHigh
      annotations:
        message: 'API server is erroring for {{ printf "%.2f" $value }}% of
          requests.'
        doc: "This alert fires if the requests error rate calculated in a 5
          minutes window was more than 5% in the last 10 minutes."
      expr: |
        sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m])) without(instance, pod)
          /
        sum(rate(apiserver_request_count{job="apiserver"}[5m])) without(instance, pod) * 100 > 5
      for: 10m
      labels:
        severity: critical
  - name: alertmanager.rules
    rules:
    - alert: AlertmanagerConfigInconsistent
      annotations:
        message: 'The configuration of the instances of the Alertmanager
          cluster {{$labels.service}} are out of sync.'
        doc: "This alert fires if the configuration of the instances of the
          Alertmanager cluster were out of sync in the last 5 minutes."
      expr: |
        count_values("config_hash", alertmanager_config_hash{job="alertmanager-main"}) BY (service) / ON(service) GROUP_LEFT() label_replace(prometheus_operator_alertmanager_spec_replicas{job="prometheus-operator"}, "service", "alertmanager-$1", "alertmanager", "(.*)") != 1
      for: 5m
      labels:
        severity: critical
    - alert: AlertmanagerDownOrMissing
      annotations:
        message: 'An unexpected number of Alertmanagers are scraped or
          Alertmanagers disappeared from discovery.'
        doc: "This alert fires if in the last 5 minutes an unexpected number of
          Alertmanagers were scraped or Alertmanagers disappered from target
          discovery."
      expr: |
        label_replace(prometheus_operator_alertmanager_spec_replicas{job="prometheus-operator"}, "job", "alertmanager-$1", "alertmanager", "(.*)") / ON(job) GROUP_RIGHT() sum(up{job="alertmanager-main"}) BY (job) != 1
      for: 5m
      labels:
        severity: critical
    - alert: AlertmanagerFailedReload
      annotations:
        message: "Reloading Alertmanager's configuration has failed for
          {{ $labels.namespace }}/{{ $labels.pod}}."
        doc: "This alert fires if the Alertmanager's configuration reload failed
          in the last 10 minutes."
      expr: |
        alertmanager_config_last_reload_successful{job="alertmanager-main"} == 0
      for: 10m
      labels:
        severity: critical
  - name: general.rules
    rules:
    - alert: TargetDown
      annotations:
        message: '{{ printf "%.2f" $value }}% of {{ $labels.job }} targets
          are down.'
        doc: "This alert fires if more than 10% of the targets were down in the
          last 10 minutes."
      expr: 100 * (count(up == 0) BY (job) / count(up) BY (job)) > 10
      for: 10m
      labels:
        severity: critical
    - alert: FdExhaustion
      annotations:
        message: '{{ $labels.job }} instance {{ $labels.instance }} will exhaust
          its file descriptors within the next 4 hours.'
        doc: "This alert fires if, based on a linear prediction on file
          descriptors usage in the last hour minutes, the instance will exhaust
          its file descriptors in 4 hours."
      expr: |
        predict_linear(instance:fd_utilization[1h], 3600 * 4) > 1
      for: 10m
      labels:
        severity: warning
    - alert: FdExhaustion
      annotations:
        message: '{{ $labels.job }} instance {{ $labels.instance }} will exhaust
          its file descriptors within the next hour.'
        doc: "This alert fires if, based on a linear prediction on file
          descriptors usage in the last 10 minutes, the instance will exhaust
          its file descriptors in one hour."
      expr: |
        predict_linear(instance:fd_utilization[10m], 3600) > 1
      for: 10m
      labels:
        severity: critical
    - alert: DeadMansSwitch
      annotations:
        message: This is a DeadMansSwitch meant to ensure that the entire
          Alerting pipeline is functional.
        doc: "This is a DeadMansSwitch meant to ensure that the entire
          Alerting Pipeline is functional."
      expr: vector(1)
      labels:
        severity: none
  - name: kube-prometheus-node-alerting.rules
    rules:
    - alert: NodeCPUSaturating
      annotations:
        message: 'CPU saturation on node {{ $labels.instance }} is too high,
          current saturation is {{ printf "%.2f" $value }}%.'
        doc: "This alert fires if, for a given instance, CPU utilisation and
          saturation were higher than 90% in the last 30 minutes."
      expr: |
       ((1 - avg by(instance) (rate(node_cpu{mode="idle"}[1m]))) > 90)
       and
       (((sum by (instance) (node_load15))
         /
       (count by (instance) (node_cpu{mode="idle"}))) * 100 > 90)
      for: 30m
      labels:
        severity: warning
    - alert: NodeCPUStuckInIOWait
      annotations:
        message: '{{ $labels.instance }} spent more than half its CPU time in
          IOWait in the last 5 minutes'
        doc: "This alert fires if CPU time in IOWait mode calculated on a 5
          minutes window for a given instance was more than 50% in the last 15
          minutes."
      expr: |
        rate(node_cpu_seconds_total{mode="iowait"}[5m]) > 0.5
      for: 15m
      labels:
        severity: warning
    - alert: NodeMemoryRunningFull
      annotations:
        message: 'Memory on node {{ $labels.instance }} is running full, current
          utilisation is {{ printf "%.2f" $value }}%.'
        doc: "This alert fires if memory utilisation on a given node was higher
          than 85% in the last 30 minutes."
      expr: |
        1 -
        sum by (instance)
        ((node_memory_MemFree +
          node_memory_Cached +
          node_memory_Buffers))
        /
        sum by (instance)
        (node_memory_MemTotal) * 100 > 85
      for: 30m
      labels:
        severity: warning
    - alert: NodeFilesystemUsageCritical
      annotations:
        message: 'Device {{ $labels.device }} of node-exporter
          {{ $labels.namespace }}/{{ $labels.pod }} on instance
          {{ $labels.instance }} has {{ printf "%0.0f" $value }}% free space.'
        doc: "This alert fires if in the last minute the filesystem usage
          was more than 85%."
      expr: |
        100 * node:node_filesystem_usage: > 85
      for: 1m
      labels:
        severity: critical
    - alert: NodeFilesystemFullInFourDays
      annotations:
        message: 'Device {{ $labels.device }} of node-exporter
          {{ $labels.namespace }}/{{ $labels.pod }} on instance
          {{ $labels.instance }} is running full within the next four days.'
        doc: "This alert fires if in the last 5 minutes the filesystem usage
          was more than 80% and, based on a linear prediction on the
          volume usage in the last 6 hours, the volume will be full in four
          days."
      expr: |
        (node:node_filesystem_usage: > 0.80)
        and
        (predict_linear(node:node_filesystem_avail:[6h], 4 * 24 * 3600) < 0)
      for: 5m
      labels:
        severity: warning
    - alert: NodeFilesystemInodeUsageCritical
      annotations:
        message: 'Node {{ $labels.instance }} has {{ printf "%0.0f" $value }}% free inodes.'
        doc: "This alert fires if the available inodes in a given
          filesystem were less than 10% in the last minute."
      expr: |
        100 * (node_filesystem_files_free{fstype=~"ext[234]|btrfs|xfs|zfs|tmpfs"}
          /
        node_filesystem_files{fstype=~"ext[234]|btrfs|xfs|zfs|tmpfs"}) < 10
      for: 1m
      labels:
        severity: critical
    - alert: NodeFilesystemInodeFullInFourDays
      annotations:
        message: 'Based on recent sampling, filesystem of instance {{ $labels.instance }}
          is expected to exhaust its inodes within four days.'
        doc: "This alert fires if, based on a linear prediction on the
          inodes usage in the last 6 hours, the filesystem will exhaust its inodes
          in four days."
      expr: |
        ((node_filesystem_files_free{fstype=~"ext[234]|btrfs|xfs|zfs|tmpfs"}
          /
        node_filesystem_files{fstype=~"ext[234]|btrfs|xfs|zfs|tmpfs"}) < 0.15)
        and
        predict_linear(node_filesystem_files_free{fstype=~"ext[234]|btrfs|xfs|zfs|tmpfs"}[6h], 4 * 24 * 3600) < 0
      for: 5m
      labels:
        severity: warning
    - alert: NodeNetworkDroppingPackets
      annotations:
        message: 'Network interface {{ $labels.device }} on node
          {{ $labels.instance }} is dropping {{ $value }} pkt/s.'
        doc: "This alerts fires if a given physical network interface was
          dropping more than 10 pkt/s in the last 30 minutes."
      expr: |
        sum by(instance,device)
          ((irate(node_network_receive_drop{device=~"eth[[:digit:]]+|en.*"}[1m])
            +
            irate(node_network_transmit_drop{device=~"eth[[:digit:]]+|en.*"}[1m]))) > 10
      for: 30m
      labels:
        severity: warning
  - name: prometheus.rules
    rules:
    - alert: PrometheusConfigReloadFailed
      annotations:
        message: "Reloading Prometheus' configuration has failed for
          {{ $labels.namespace }}/{{ $labels.pod }}."
        doc: "This alert fires if Prometheus's configuration failed to reload in
          the last 10 minutes."
      expr: |
        prometheus_config_last_reload_successful{job="prometheus-k8s"} == 0
      for: 10m
      labels:
        severity: critical
    - alert: PrometheusNotificationQueueRunningFull
      annotations:
        message: "Prometheus' alert notification queue is running full for
          {{ $labels.namespace }}/{{ $labels.pod }}."
        doc: "This alert fires if Prometheus's alert notification queue is
          running full in the next 30 minutes, based on a linear prediction on
          the usage in the last 5 minutes."
      expr: |
        predict_linear(prometheus_notifications_queue_length{job="prometheus-k8s"}[5m], 60 * 30) > prometheus_notifications_queue_capacity{job="prometheus-k8s"}
      for: 10m
      labels:
        severity: critical
    - alert: PrometheusErrorSendingAlerts
      annotations:
        message: 'Errors while sending alerts from Prometheus
          {{ $labels.namespace }}/{{ $labels.pod }} to Alertmanager
          {{ $labels.alertmanager }}.'
        doc: "This alert fires if the error rate calculated in a 5 minutes time
          windows was more than 1% in the last 10 minutes."
      expr: |
        rate(prometheus_notifications_errors_total{job="prometheus-k8s"}[5m]) / rate(prometheus_notifications_sent_total{job="prometheus-k8s"}[5m]) > 0.01
      for: 10m
      labels:
        severity: critical
    - alert: PrometheusErrorSendingAlerts
      annotations:
        message: 'Errors while sending alerts from Prometheus
         {{ $labels.namespace }}/{{ $labels.pod }} to Alertmanager
         {{$labels.alertmanager }}.'
        doc: "This alert fires if the error rate calculated in a 5 minutes time
          windows was more than 3% in the last 10 minutes."
      expr: |
        rate(prometheus_notifications_errors_total{job="prometheus-k8s"}[5m]) / rate(prometheus_notifications_sent_total{job="prometheus-k8s"}[5m]) > 0.03
      for: 10m
      labels:
        severity: critical
    - alert: PrometheusNotConnectedToAlertmanagers
      annotations:
        message: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not
          connected to any Alertmanagers.'
        doc: "This alert fires if Prometheus was not connected to at least one
          Alertmanager in the last 10 minutes."
      expr: |
        prometheus_notifications_alertmanagers_discovered{job="prometheus-k8s"} < 1
      for: 10m
      labels:
        severity: critical
    - alert: PrometheusTSDBReloadsFailing
      annotations:
        message: '{{ $labels.job }} at {{ $labels.instance }} had
          {{ $value | humanize }} reload failures over the last four hours.'
        doc: "This alert fires if Prometheus had any failure to reload data
          blocks from disk in the last 12 hours."
      expr: |
        increase(prometheus_tsdb_reloads_failures_total{job="prometheus-k8s"}[2h]) > 0
      for: 12h
      labels:
        severity: critical
    - alert: PrometheusTSDBCompactionsFailing
      annotations:
        message: '{{ $labels.job }} at {{ $labels.instance }} had
          {{ $value | humanize }} compaction failures over the last four hours.'
        doc: "This alert fires if Prometheus had any failure to compact sample
          blocks in the last 12 hours."
      expr: |
        increase(prometheus_tsdb_compactions_failed_total{job="prometheus-k8s"}[2h]) > 0
      for: 12h
      labels:
        severity: critical
    - alert: PrometheusTSDBWALCorruptions
      annotations:
        message: '{{ $labels.job }} at {{ $labels.instance }} has a corrupted
          write-ahead log (WAL).'
        doc: "This alert fires if Prometheus had detected any corruption in the
          write-ahead log in the last 4 hours."
      expr: |
        tsdb_wal_corruptions_total{job="prometheus-k8s"} > 0
      for: 4h
      labels:
        severity: critical
    - alert: PrometheusNotIngestingSamples
      annotations:
        message: "Prometheus {{ $labels.namespace }}/{{ $labels.pod}} isn't
          ingesting samples."
        doc: "This alert fires if Prometheus sample ingestion rate calculated on
          a 5 minutes time window was below or equal to 0 in the last 10
          minutes, i.e. Prometheus is failing to ingest samples."
      expr: |
        rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-k8s"}[5m]) <= 0
      for: 10m
      labels:
        severity: critical
    - alert: PrometheusTargetScrapesDuplicate
      annotations:
        message: '{{ $labels.namespace }}/{{ $labels.pod }} has many samples
          rejected due to duplicate timestamps but different values.'
        doc: "This alert fires if Prometheus was discarding many samples due to
          duplicated timestamps but different values in the last 10 minutes."
      expr: |
        increase(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-k8s"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
